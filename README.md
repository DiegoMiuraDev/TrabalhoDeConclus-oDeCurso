# Projeto TCC: Reconhecimento AutomÃ¡tico de Libras com IA

## ğŸ“‹ VisÃ£o Geral

Este projeto desenvolve um **sistema de reconhecimento automÃ¡tico de sinais estÃ¡ticos da LÃ­ngua Brasileira de Sinais (Libras)** utilizando InteligÃªncia Artificial e VisÃ£o Computacional.

O foco atual estÃ¡ no reconhecimento de um subconjunto de sinais (principalmente **vogais A, E, I, O, U**), com infraestrutura preparada para escalar para as **24 letras do alfabeto de Libras**.

### ğŸ¯ Objetivo Geral
Criar uma aplicaÃ§Ã£o capaz de reconhecer, em tempo real atravÃ©s de uma webcam, os sinais manuais correspondentes Ã s letras do alfabeto de Libras, contribuindo para inclusÃ£o e acessibilidade da comunidade surda.

---

## ğŸ—ï¸ Estrutura do Projeto

Estrutura real do repositÃ³rio (atualizada):

```
tcc/
â”œâ”€â”€ README.md                      # Este arquivo (visÃ£o geral)
â”œâ”€â”€ RESUMO_PROJETO.md             # Resumo tÃ©cnico detalhado do TCC
â”œâ”€â”€ requirements.txt              # DependÃªncias Python
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.py                 # ConfiguraÃ§Ãµes centralizadas (dataset, modelo, treino, realtime)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset_loader.py     # Carregamento e organizaÃ§Ã£o do dataset
â”‚   â”‚   â””â”€â”€ preprocessing.py      # PrÃ©-processamento e preparaÃ§Ã£o p/ MobileNetV2
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ helpers.py            # FunÃ§Ãµes auxiliares (GPU, infos de sistema, imagens)
â”‚   â”‚   â””â”€â”€ kaggle_setup.py       # AutomaÃ§Ã£o da configuraÃ§Ã£o/download do Kaggle
â”‚   â””â”€â”€ visualization/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ plots.py              # GrÃ¡ficos (distribuiÃ§Ã£o, histÃ³rico, matriz de confusÃ£o etc.)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_model.py            # Pipeline completo de treinamento + avaliaÃ§Ã£o
â”‚   â””â”€â”€ real_time_demo.py         # AplicaÃ§Ã£o de reconhecimento em tempo real via webcam
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb # ExploraÃ§Ã£o detalhada do dataset
â”‚   â””â”€â”€ 01_data_exploration_simples.ipynb # VersÃ£o simplificada da exploraÃ§Ã£o
â”œâ”€â”€ libras_recognition_phase1.ipynb # Notebook geral da fase 1
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ESTRUTURA_PROJETO.md      # Detalhes da organizaÃ§Ã£o de mÃ³dulos
â”‚   â””â”€â”€ FASE1_GUIA.md             # Guia completo da fase 1 (passo a passo)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ libras_brasileiro_best.h5 # Modelo treinado (melhor versÃ£o atual)
â”‚   â””â”€â”€ libras_classes.bak        # Backup/classes do modelo
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ keras_model.h5            # Modelo em formato Keras (versÃ£o exportada)
â”‚   â”œâ”€â”€ model_unquant.tflite      # Modelo convertido para TFLite (uso embarcado)
â”‚   â””â”€â”€ test_images/              # Conjunto de teste com imagens reais (A, E, I, O, U)
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ matriz_confusao.npy       # Matriz de confusÃ£o gerada pelo treinamento
â”‚   â”œâ”€â”€ metricas_precisao.csv     # MÃ©tricas em CSV
â”‚   â””â”€â”€ metricas_precisao.md      # Resumo das mÃ©tricas por classe
â”œâ”€â”€ logs/                         # Logs de execuÃ§Ã£o/treinamento
â””â”€â”€ venv*, venv312_mp/            # Ambientes virtuais (nÃ£o necessÃ¡rios para uso em Colab)
```

Para detalhes finos de cada mÃ³dulo, consulte tambÃ©m `docs/ESTRUTURA_PROJETO.md`.

---

## ğŸš€ Tecnologias Utilizadas

- **Linguagem:** Python 3.8+
- **Framework de IA:** TensorFlow / Keras
- **Modelo Base:** MobileNetV2 (Transfer Learning)
- **VisÃ£o Computacional:** OpenCV
- **AnÃ¡lise de Dados:** NumPy, Pandas
- **VisualizaÃ§Ã£o:** Matplotlib, Seaborn, Plotly
- **Dataset Base:** Libras MNIST (Kaggle)
- **Ambiente de Desenvolvimento:** Google Colab (GPU) e ambiente local com virtualenv

---

## ğŸ“Š Dataset

- **Nome:** Libras MNIST  
- **Fonte:** Kaggle (`https://www.kaggle.com/datasets/datamoon/libras-mnist`)  
- **Classes (objetivo final):** 24 letras (Aâ€“X) do alfabeto de Libras  
- **Formato original:** Imagens 28Ã—28 em escala de cinza  
- **Formato para o modelo:** Imagens 224Ã—224 RGB (3 canais), compatÃ­veis com MobileNetV2  
- **ConfiguraÃ§Ã£o do dataset no cÃ³digo:** ver `DATASET_CONFIG` em `configs/config.py`.

O diretÃ³rio `dataset/test_images/` contÃ©m um **conjunto de teste prÃ¡tico** com imagens reais das letras **A, E, I, O, U**, usado na avaliaÃ§Ã£o manual e na validaÃ§Ã£o da demo em tempo real.

---

## ğŸ¯ Fases do Projeto

### âœ… Fase 1 â€“ AnÃ¡lise e ExploraÃ§Ã£o dos Dados
- ConfiguraÃ§Ã£o do ambiente (Colab + GPU / ambiente local)
- Download e carregamento do dataset Libras MNIST via Kaggle
- AnÃ¡lise exploratÃ³ria e visualizaÃ§Ã£o de amostras
- PrÃ©-processamento das imagens para MobileNetV2
- DivisÃ£o em treino / validaÃ§Ã£o / teste

**DocumentaÃ§Ã£o e materiais:**
- Notebook: `notebooks/01_data_exploration.ipynb`
- Guia: `docs/FASE1_GUIA.md`
- Estrutura: `docs/ESTRUTURA_PROJETO.md`

---

### ğŸ”„ Fase 2 â€“ PrÃ©-processamento, Treinamento e AvaliaÃ§Ã£o

Status: **implementada e com resultados iniciais gerados**.

Passos principais (automatizados em `scripts/train_model.py`):
- Carregamento e preparaÃ§Ã£o dos dados (`src/data/dataset_loader.py` e `preprocessing.py`)
- ConstruÃ§Ã£o do modelo MobileNetV2 com Transfer Learning (`create_mobilenet_model`)
- Treinamento com callbacks (EarlyStopping, ReduceLROnPlateau, ModelCheckpoint)
- AvaliaÃ§Ã£o em treino/validaÃ§Ã£o/teste
- GeraÃ§Ã£o de histÃ³rico de treinamento e matriz de confusÃ£o
- Salvamento de mÃ©tricas em `results/`

**Resumo das mÃ©tricas atuais (vogais A, E, I, O, U â€“ ver `results/metricas_precisao.md`):**
- **AcurÃ¡cia geral:** ~50,5% no conjunto de teste de 5 classes  
- **PrecisÃ£o por classe (exemplos):**
  - O: precisÃ£o â‰ˆ 0,96, F1 â‰ˆ 0,94
  - E: precisÃ£o â‰ˆ 0,83
  - U: recall alto (â‰ˆ 0,86), mas precisÃ£o moderada

Esses resultados mostram desempenho jÃ¡ sÃ³lido para algumas classes (como **O**), porÃ©m com espaÃ§o para melhorar a separaÃ§Ã£o entre todas as vogais e, futuramente, escalar para as 24 letras.

---

### ğŸ¥ Fase 3 â€“ AplicaÃ§Ã£o em Tempo Real com Webcam

Status: **script implementado e pronto para uso com modelo treinado**.

Funcionalidades principais (`scripts/real_time_demo.py`):
- Captura de vÃ­deo em tempo real via OpenCV
- PrÃ©-processamento do frame (grayscale â†’ resize 224Ã—224 â†’ RGB â†’ normalizaÃ§Ã£o)
- PrediÃ§Ã£o com o modelo treinado (`models/libras_model.h5` ou similares)
- ExibiÃ§Ã£o da letra prevista, confianÃ§a e status (ALTO/BAIXO) sobre o vÃ­deo
- Controle via teclado: `q` para sair, `s` para salvar frames

ConfiguraÃ§Ãµes como Ã­ndice da cÃ¢mera, resoluÃ§Ã£o, limiar de confianÃ§a e intervalo de prediÃ§Ã£o sÃ£o definidas em `REALTIME_CONFIG` (`configs/config.py`).

---

## ğŸ› ï¸ Como Usar o Projeto

### 1. Clonar o repositÃ³rio e instalar dependÃªncias

```bash
git clone <seu-repositorio>
cd tcc

# (opcional) criar ambiente virtual
python -m venv venv
venv\Scripts\activate  # Windows

# instalar dependÃªncias principais e de desenvolvimento
pip install -r requirements.txt
```

> Em Google Colab, normalmente basta copiar os trechos de instalaÃ§Ã£o do `requirements.txt` ou usar as versÃµes jÃ¡ disponÃ­veis no ambiente.

### 2. Configurar Kaggle (para baixar o Libras MNIST)

1. Crie uma conta no Kaggle.  
2. Em *Account* â†’ *Create New API Token*, baixe o arquivo `kaggle.json`.  
3. No ambiente local ou Colab, coloque o `kaggle.json` no local esperado (ver `COLAB_CONFIG` em `configs/config.py`).  
4. Execute o script de setup (se estiver usando o fluxo automatizado):

```bash
python -m src.utils.kaggle_setup
```

### 3. Executar a Fase 1 (exploraÃ§Ã£o de dados)

```bash
jupyter notebook notebooks/01_data_exploration.ipynb
```

Ou, em Colab, faÃ§a upload do repositÃ³rio/notebook e execute cÃ©lula a cÃ©lula conforme `docs/FASE1_GUIA.md`.

### 4. Treinar o modelo (Fase 2)

```bash
python scripts/train_model.py
```

SaÃ­das esperadas:
- Modelo salvo em `models/libras_model.h5` (ou nome equivalente configurado)
- HistÃ³rico de treinamento: `results/training_history.npy` e PNG correspondente
- MÃ©tricas em `results/metrics.npy` e arquivos auxiliares em `results/`
- Matriz de confusÃ£o salva em `results/confusion_matrix.png` e `matriz_confusao.npy`

### 5. Rodar a demo em tempo real (Fase 3)

Certifique-se de que existe um modelo treinado em `models/` compatÃ­vel com o script.

```bash
python scripts/real_time_demo.py
```

InstruÃ§Ãµes na tela:
- Posicione a mÃ£o em frente Ã  cÃ¢mera
- FaÃ§a o sinal da letra desejada
- Pressione **`q`** para encerrar, **`s`** para salvar um frame

---

## ğŸ“ˆ Resultados e PrÃ³ximos Passos

### Resultados atuais
- **AcurÃ¡cia geral (5 classes â€“ A, E, I, O, U):** ~50,5%  
- **Boas mÃ©tricas individuais** para a letra **O** (F1 â‰ˆ 0,94) e desempenho intermediÃ¡rio para E e U.  
- **Matriz de confusÃ£o** e mÃ©tricas detalhadas disponÃ­veis em `results/metricas_precisao.md` e `matriz_confusao.npy`.

### PrÃ³ximos passos sugeridos
- Refinar o prÃ©-processamento e *data augmentation* para reduzir confusÃµes entre letras semelhantes.
- Ajustar hiperparÃ¢metros (learning rate, batch_size, epochs) e experimentar *fine-tuning* de camadas da MobileNetV2.
- Ampliar o dataset para incluir todas as 24 letras e novos fundos/iluminaÃ§Ãµes.
- Otimizar o modelo para execuÃ§Ã£o embarcada (uso da versÃ£o `.tflite` em dispositivos mÃ³veis ou edge).

---

## ğŸ“š DocumentaÃ§Ã£o Complementar

- `RESUMO_PROJETO.md` â€“ resumo tÃ©cnico completo do TCC.
- `docs/ESTRUTURA_PROJETO.md` â€“ detalhes da organizaÃ§Ã£o de pastas e mÃ³dulos.
- `docs/FASE1_GUIA.md` â€“ passo a passo detalhado da fase 1.
- `results/metricas_precisao.md` â€“ mÃ©tricas atuais por classe.

---

## ğŸ¤ ContribuiÃ§Ã£o

Este Ã© um projeto de **Trabalho de ConclusÃ£o de Curso (TCC)**. SugestÃµes de melhoria, comentÃ¡rios e contribuiÃ§Ãµes acadÃªmicas sÃ£o bem-vindas.

Para contato ou dÃºvidas, utilize os canais definidos no texto do TCC (e-mail institucional, orientador etc.).

---

## ğŸ“„ LicenÃ§a

Este projeto Ã© destinado a **fins educacionais e de pesquisa**, podendo ser reutilizado e adaptado para estudos semelhantes, desde que citada a autoria original.

---

**Desenvolvido por:** [Seu Nome]  
**Orientador:** [Nome do Orientador]  
**InstituiÃ§Ã£o:** [Nome da InstituiÃ§Ã£o]  
**Ano:** 2024
